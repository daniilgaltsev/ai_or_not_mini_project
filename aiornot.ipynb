{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Goals\n","\n","The advances in AI image generation have made big strides in recent times. It is at a point where these generated images can pass as real ones if not enough attention is paid to trying to dicsern them. This can be a problem for many reasons starting from spam and ending with disinformation campaigns. \n","\n","The goal of this project is to train a model to detect these images. Since it's quite a novel task, we don't have a good idea of what the expected performance should be. This will require establishing a baseline and determining what kind of errors are acceptable. In addition, this project will also serve as a practice for working with parquet files and using an experiment tracking tool."]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This notebook was used mostly in a Kaggle environment, so either the data needs to be uploaded there (train files) or the references to Kaggle need to be changed (imports, paths and W&B credentials)."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:32:51.672744Z","iopub.status.busy":"2023-04-18T15:32:51.672232Z","iopub.status.idle":"2023-04-18T15:32:51.710801Z","shell.execute_reply":"2023-04-18T15:32:51.709600Z","shell.execute_reply.started":"2023-04-18T15:32:51.672698Z"},"trusted":true},"outputs":[],"source":["LOG_CHECKPOINTS = False"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:32:51.951579Z","iopub.status.busy":"2023-04-18T15:32:51.950328Z","iopub.status.idle":"2023-04-18T15:32:51.960455Z","shell.execute_reply":"2023-04-18T15:32:51.959398Z","shell.execute_reply.started":"2023-04-18T15:32:51.951520Z"},"trusted":true},"outputs":[],"source":["INTERACTIVE_MODE = False\n","DEBUG_MODE = False\n","REDUCED_DATASET_MODE = False\n","\n","SWEEP_MODE = False\n","CREATE_NEW_SWEEP = False\n","\n","TRAIN_CANDIDATES_MODE = False\n","\n","TRAIN_FINAL_MODE = True\n","TEST_FINAL_MODE = True\n","\n","\n","# sweep_id = \"qxp2wgy1\"  # full (512, ~18000)\n","# sweep_id = \"fxz1z4nx\"  # reduced (256, 5000)\n","# sweep_id = \"kr39572n\" # reduced with transforms\n","# sweep_id = \"9y8abom7\" # reduced with transforms bayes\n","# sweep_id = \"ukwbdg8y\" # reduced final\n","sweep_id = None  # Add sweep_id to use, otherwise will create a new sweep with default config\n","if SWEEP_MODE and not CREATE_NEW_SWEEP:\n","    if sweep_id is None:\n","        raise ValueError(\"Specify the sweep id to use or allow to create a new sweep\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:32:52.130447Z","iopub.status.busy":"2023-04-18T15:32:52.129948Z","iopub.status.idle":"2023-04-18T15:33:06.019944Z","shell.execute_reply":"2023-04-18T15:33:06.018819Z","shell.execute_reply.started":"2023-04-18T15:32:52.130413Z"},"trusted":true},"outputs":[],"source":["import gc\n","import io\n","import json\n","from pathlib import Path\n","\n","\n","import h5py\n","from kaggle_secrets import UserSecretsClient\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import pyarrow\n","import pyarrow.dataset as ds\n","import pyarrow.parquet as pq\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from sklearn.model_selection import StratifiedKFold\n","import timm\n","import torch\n","import torch.nn as nn\n","import torchmetrics\n","import torchvision\n","from torchvision.transforms import (\n","    RandomResizedCrop, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip,\n","    ColorJitter, RandomAdjustSharpness,\n","    ToTensor, Compose, Normalize\n",")\n","from torchvision.transforms.functional import to_tensor\n","from tqdm.notebook import tqdm\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:06.022616Z","iopub.status.busy":"2023-04-18T15:33:06.022145Z","iopub.status.idle":"2023-04-18T15:33:09.526017Z","shell.execute_reply":"2023-04-18T15:33:09.524810Z","shell.execute_reply.started":"2023-04-18T15:33:06.022577Z"},"trusted":true},"outputs":[],"source":["user_secrets = UserSecretsClient()\n","wandb_apikey = user_secrets.get_secret(\"wandb_apikey\")\n","wandb.login(key=wandb_apikey)\n","\n","use_gpu = torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"markdown","metadata":{},"source":["The data for this project comes from the dataset on Hugging Face called [aiornot](https://huggingface.co/datasets/competitions/aiornot). It was originally used in a competition with the same name. The data itself consists of train images, labels for those images and test images without labels.\n","\n","I am going to be using only the train images and I am going to split them into my own train, validation and test sets.\n"," "]},{"cell_type":"markdown","metadata":{},"source":["## Data Loading and EDA"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Function and Class Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_DIR = Path(\"/kaggle/input/ai-or-not\")\n","\n","RAW_IMAGE_SHAPE = (512, 512, 3)\n","if REDUCED_DATASET_MODE:\n","    RAW_IMAGE_SHAPE = (256, 256, 3)\n","PREPROCESSING_BATCH_SIZE = 1024"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:09.528807Z","iopub.status.busy":"2023-04-18T15:33:09.527787Z","iopub.status.idle":"2023-04-18T15:33:09.534052Z","shell.execute_reply":"2023-04-18T15:33:09.532933Z","shell.execute_reply.started":"2023-04-18T15:33:09.528763Z"},"trusted":true},"outputs":[],"source":["def read_data():\n","    parquets_filenames = list(DATA_DIR.glob(\"train*\"))\n","    parquets = [pq.read_table(p) for p in parquets_filenames]\n","    data_table = ds.dataset(parquets_filenames, format=\"parquet\").to_table()\n","    return data_table\n","\n","def show_target_distribution():\n","    label_counts = labels[\"label\"].value_counts()\n","    plt.title(\"Distribution of target\")\n","    sns.barplot(y=label_counts.values, x=label_counts.index)\n","    plt.show()\n","\n","def decode_image(bytes_):\n","    return np.array(\n","        Image.open(io.BytesIO(bytes_)).resize(RAW_IMAGE_SHAPE[:2]),\n","        dtype=np.uint8\n","    )\n","\n","def decode_batch(image_batch, shape=RAW_IMAGE_SHAPE):\n","    images = image_batch.to_pydict()[\"image\"]\n","    batch_size = len(images)\n","    images_array = np.empty((batch_size, *shape), dtype=np.uint8)\n","    for idx, image in enumerate(images):\n","        images_array[idx] = decode_image(image[\"bytes\"])\n","    return images_array"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Basic Dataset Info"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since data is stored in Hugging Face's datasets, there are convinience functions in their library to download datasets, but I wanted to practice working with parquet files myself.\n","\n","The images are split into train set containing labels and test set without labels. Since we need to evaluate the model ourselves, we are going to use only the train set and manually create the splits.\n","\n","The train set consists of two parquet files."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:09.537905Z","iopub.status.busy":"2023-04-18T15:33:09.537542Z","iopub.status.idle":"2023-04-18T15:33:13.336818Z","shell.execute_reply":"2023-04-18T15:33:13.335685Z","shell.execute_reply.started":"2023-04-18T15:33:09.537879Z"},"trusted":true},"outputs":[],"source":["data_table = read_data()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.339353Z","iopub.status.busy":"2023-04-18T15:33:13.338961Z","iopub.status.idle":"2023-04-18T15:33:13.345100Z","shell.execute_reply":"2023-04-18T15:33:13.343782Z","shell.execute_reply.started":"2023-04-18T15:33:13.339317Z"},"trusted":true},"outputs":[],"source":["if DEBUG_MODE:\n","    data_table = data_table.slice(length=1000)\n","elif REDUCED_DATASET_MODE:\n","    data_table = data_table.slice(length=5000)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.348356Z","iopub.status.busy":"2023-04-18T15:33:13.347306Z","iopub.status.idle":"2023-04-18T15:33:13.356851Z","shell.execute_reply":"2023-04-18T15:33:13.355551Z","shell.execute_reply.started":"2023-04-18T15:33:13.348320Z"},"trusted":true},"outputs":[],"source":["print(f\"Columns: {data_table.column_names}\")\n","print(f\"Number of examples: {len(data_table)}\")"]},{"cell_type":"markdown","metadata":{},"source":["As can be seen, the dataset contains 18618 examples and 3 columns `id`, `image` and `label`."]},{"cell_type":"markdown","metadata":{},"source":["First, we will take a look at the distribution of labels to check how unbalanced the dataset is."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.359597Z","iopub.status.busy":"2023-04-18T15:33:13.358742Z","iopub.status.idle":"2023-04-18T15:33:13.384138Z","shell.execute_reply":"2023-04-18T15:33:13.382955Z","shell.execute_reply.started":"2023-04-18T15:33:13.359557Z"},"trusted":true},"outputs":[],"source":["labels = data_table.select([\"id\", \"label\"]).to_pandas()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.387082Z","iopub.status.busy":"2023-04-18T15:33:13.386246Z","iopub.status.idle":"2023-04-18T15:33:13.393549Z","shell.execute_reply":"2023-04-18T15:33:13.392676Z","shell.execute_reply.started":"2023-04-18T15:33:13.387036Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE:\n","    show_target_distribution()"]},{"cell_type":"markdown","metadata":{},"source":["The bar plot shows that the dataset is fairly balanced and we can use this distribution as is when splitting."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Assessing the Task"]},{"cell_type":"markdown","metadata":{},"source":["Now, we need to take a look at the images themselves to assess the difficulty of the task and what kind of transformations are needed or we can use."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.411200Z","iopub.status.busy":"2023-04-18T15:33:13.410692Z","iopub.status.idle":"2023-04-18T15:33:13.418604Z","shell.execute_reply":"2023-04-18T15:33:13.417511Z","shell.execute_reply.started":"2023-04-18T15:33:13.411171Z"},"trusted":true},"outputs":[],"source":["images_table = data_table.select([\"image\"])\n","if INTERACTIVE_MODE:\n","    decoded_batch = decode_batch(next(iter(images_table.to_batches(PREPROCESSING_BATCH_SIZE))))\n","    decoded_batch.shape"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.423145Z","iopub.status.busy":"2023-04-18T15:33:13.420659Z","iopub.status.idle":"2023-04-18T15:33:13.431600Z","shell.execute_reply":"2023-04-18T15:33:13.430349Z","shell.execute_reply.started":"2023-04-18T15:33:13.423100Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE:\n","    np.random.seed(0)\n","    samples = np.random.randint(len(decoded_batch), size=(16,))\n","\n","    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))\n","    fig.suptitle('Sample Images')\n","\n","    for i in range(4):\n","        for j in range(4):\n","            idx = samples[i * 4 + j]\n","            axis = axes[i, j]\n","            axis.imshow(decoded_batch[idx])\n","            axis.axis('off')\n","            axis.set_title(labels.loc[idx].label)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The images themselves seem to be fine and there is no need for any special preprocessing. However, the labels are just `0` and `1`, so, we need to make sure which label is which. Label `1` seems to correspond with generated images, but to be certain we are going to take a closer look at them."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.435230Z","iopub.status.busy":"2023-04-18T15:33:13.433796Z","iopub.status.idle":"2023-04-18T15:33:13.444892Z","shell.execute_reply":"2023-04-18T15:33:13.443774Z","shell.execute_reply.started":"2023-04-18T15:33:13.435185Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE:\n","    potential_ai_labels = labels[labels.label == 1]\n","\n","    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n","    fig.suptitle('Sample Images')\n","\n","    for i in range(3):\n","        for j in range(3):\n","            label = potential_ai_labels.iloc[i * 3 + j]\n","            idx = label.name\n","            axis = axes[i, j]\n","            axis.imshow(decoded_batch[idx])\n","            axis.axis('off')\n","            axis.set_title(label.label)\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The first image has weirdly placed eyes and a neck at a wrong angle. That confirms that this class is generated images. Additionally, other images have this weird `smoothness` and there is also a man with 6 (and a half?) fingers."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Manual Benchmark"]},{"cell_type":"markdown","metadata":{},"source":["The task is pretty novel, so, I have no idea what a good level performance can be. One way to check is to research what other people were able to achieve. The other way is to do a manual benchmark. We can manually sample some number of images and try to classify them ourselves. The results will indicate a human-level performance, which is generally a pretty good benchmark."]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2023-04-18T15:33:13.447285Z","iopub.status.busy":"2023-04-18T15:33:13.446731Z","iopub.status.idle":"2023-04-18T15:33:13.457284Z","shell.execute_reply":"2023-04-18T15:33:13.455126Z","shell.execute_reply.started":"2023-04-18T15:33:13.447245Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE and not DEBUG_MODE:\n","    np.random.seed(1)\n","    NUMBER_OF_IMAGES_TO_GUESS = 50\n","    samples = np.random.randint(len(decoded_batch), size=(NUMBER_OF_IMAGES_TO_GUESS,))\n","    # samples = [ # The samples I've guessed on\n","    #     37, 235, 908, 72, 767, 905, 715, 645, 847, 960, 144, 129, 972, 583, 749, 508, 390,\n","    #     281, 178, 276, 254, 357, 914, 468, 907, 252, 490, 668, 925, 398, 562, 580, 215, 983,\n","    #     753, 503, 478, 864, 86, 141, 393, 7, 319, 829, 534, 313, 513, 896, 316, 209\n","    # ]\n","\n","\n","    guess = []\n","    true = []\n","\n","    for i in range(NUMBER_OF_IMAGES_TO_GUESS):\n","        idx = samples[i]\n","        label = labels.loc[idx].label\n","        true.append(label)\n","        plt.imshow(decoded_batch[idx])\n","        plt.axis('off')\n","        plt.show()\n","        guess.append(int(input(\"AI=1, Human=0:\")))\n","        answer = \"AI\" if label else \"Human\"\n","        print(\"It was\", answer)\n","\n","    guess = np.array(guess)\n","    true = np.array(true)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.459860Z","iopub.status.busy":"2023-04-18T15:33:13.459401Z","iopub.status.idle":"2023-04-18T15:33:13.474474Z","shell.execute_reply":"2023-04-18T15:33:13.473061Z","shell.execute_reply.started":"2023-04-18T15:33:13.459821Z"},"trusted":true},"outputs":[],"source":["# My Score\n","# Accuracy = 0.8400\n","# AI Accuracy = 0.9545\n","# Human Accuracy = 0.7500\n","# Recall = 0.9545\n","# Precision = 0.7500\n","#\n","#    \\/      \\/        \\/      \\/      \\/  \\/      \\/          \\/\n","# [0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0]: true\n","# [0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0]: guess\n","\n","if INTERACTIVE_MODE and not DEBUG_MODE:\n","    print(true)\n","    print(guess)\n","    ai_mask = (true == 1)\n","    human_mask = (true == 0)\n","    print((\n","        f\"Accuracy = {accuracy_score(true, guess):.4f}\\n\"\n","        f\"Accuracy on AI images = {accuracy_score(true[ai_mask], guess[ai_mask]):.4f}\\n\"\n","        f\"Accuracy on `human` images = {accuracy_score(true[human_mask], guess[human_mask]):.4f}\\n\"\n","        f\"Recall = {recall_score(true, guess):.4f}\\n\"\n","        f\"Precision = {precision_score(true, guess):.4f}\"\n","    ))\n","    # NOTE: AI Accuracy and Precision are always equal.\n","else:\n","    print(\"Accuracy = 0.8400\")\n","    print(\"Accuracy on AI images = 0.9545\")\n","    print(\"Accuracy on `human` images = 0.7500\")\n","    print(\"Recall = 0.9545\")\n","    print(\"Precision = 0.7500\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["My results surprised me in 2 ways:\n","\n","- How intuitive it is to tell that a generated image is generated\n","- How quickly I was able to get better at it\n","\n","At first, it was pretty difficult to tell the difference, but towards the end I got a sense of the overall generated style of `strangeness` and `smoothness`. I wouldn't be surprised that with a bit more practice, I would be able to get much higher accuracy (and, technically, the last images are after my 'training', and accuracy there is 100%).\n","\n","Overall, I think the task should be pretty easy for a deep learning model and we should expect to get a really high recall with the precision being a bit behind. This error tradeoff seems to be reasonable for some possible types of applications, but for the concrete cases, it would depend on the cost of false positives and false negatives and thus require more data processing such as oversampling, collecting more data focusing on the type of errors we want to reduce, and data augmentation for the more difficult cases."]},{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Function and Class Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SCRATCH = Path(\"/kaggle/scratch\")\n","SCRATCH.mkdir(parents=True, exist_ok=True)\n","PREPROCESSED = SCRATCH / \"preprocessed.hdf5\"\n","\n","IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","IMAGENET_STD = [0.229, 0.224, 0.225]\n","PREPROCESS_TRANSFORM = Compose([\n","    ToTensor(),\n","    Normalize(IMAGENET_MEAN, IMAGENET_STD, inplace=False)\n","])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.476690Z","iopub.status.busy":"2023-04-18T15:33:13.476041Z","iopub.status.idle":"2023-04-18T15:33:13.487658Z","shell.execute_reply":"2023-04-18T15:33:13.486845Z","shell.execute_reply.started":"2023-04-18T15:33:13.476652Z"},"trusted":true},"outputs":[],"source":["def split_data(labels):\n","    sgkf = StratifiedKFold(shuffle=True, n_splits=10, random_state=0)\n","    folds = sgkf.split(X=labels[[\"id\"]], y=labels[\"label\"])\n","    for i, (train_index, test_index) in enumerate(folds):\n","        labels.loc[test_index, [\"fold\"]] = i\n","\n","    labels[\"split\"] = \"train\"\n","    labels.loc[labels[\"fold\"] == 0, [\"split\"]] = \"valid\" \n","    labels.loc[(labels[\"fold\"] == 1) | (labels[\"fold\"] == 2), [\"split\"]] = \"test\" \n","\n","    if INTERACTIVE_MODE:\n","        to_plot = {\"split\": [], \"target\": [], \"count\": []}\n","        for split in [\"train\", \"test\", \"valid\"]:\n","            split_counts = labels.loc[labels[\"split\"] == split, [\"label\"]].value_counts()\n","            for idx in range(2):\n","                to_plot[\"split\"].append(split)\n","                to_plot[\"count\"].append(split_counts.iloc[idx])\n","                to_plot[\"target\"].append(split_counts.index.get_level_values(\"label\")[idx])\n","        sns.barplot(data=to_plot, x=\"target\", y=\"count\", hue=\"split\")\n","        plt.title(\"Label distribution in splits\")\n","        plt.show()\n","    \n","    return labels\n","\n","\n","def preprocess_splits():\n","    # (                       size, disk-to-gpu speed)\n","    # Storing as float:       55GB, ~0.63it/s\n","    # Storing as uint8:       14GB, ~2.00it/s\n","    # Storing as uint8 + lzf: 12GB, ~0.02it/s\n","    with h5py.File(PREPROCESSED, \"w\") as file:\n","        for split in [\"train\", \"valid\", \"test\"]:\n","            split_labels = labels[labels[\"split\"] == split]\n","            split_images_table = images_table.take(split_labels.index.values)\n","            total = (len(split_images_table) -  1) // PREPROCESSING_BATCH_SIZE + 1\n","            split_table = data_table.select([\"image\"]).take(split_labels.index.values)\n","            split_group = file.create_group(split)\n","            image_dataset = split_group.create_dataset(\n","                \"image\",\n","                shape=(len(split_images_table), *RAW_IMAGE_SHAPE),\n","                dtype=np.uint8,\n","            )\n","            batches = tqdm(\n","                enumerate(split_images_table.to_batches(PREPROCESSING_BATCH_SIZE)),\n","                desc=f\"Preprocessing {split} images\",\n","                mininterval=1, total=total\n","            )\n","            for idx, batch in batches:\n","                start = idx * PREPROCESSING_BATCH_SIZE\n","                end = start + PREPROCESSING_BATCH_SIZE\n","    #             image_dataset[start:end] = preprocess_batch(decode_batch(batch))\n","                image_dataset[start:end] = decode_batch(batch)\n","\n","            split_group.create_dataset(\n","                \"target\",\n","                dtype=np.int64,\n","                data=split_labels.label.values.astype(np.int64)\n","            )\n","\n","\n","def preprocess_image(image):\n","    return PREPROCESS_TRANSFORM(image)\n","\n","\n","def get_train_transforms(max_rotation, max_brightness, max_contrast, sharpness_p, flip_p, use_crop):\n","    transforms = nn.Sequential(\n","        RandomResizedCrop(RAW_IMAGE_SHAPE[:2], antialias=True),\n","        RandomRotation(max_rotation),\n","        RandomHorizontalFlip(p=flip_p),\n","        RandomVerticalFlip(p=flip_p),\n","        ColorJitter(brightness=max_brightness, contrast=max_contrast),\n","        RandomAdjustSharpness(sharpness_factor=1.5, p=sharpness_p),\n","        Normalize(IMAGENET_MEAN, IMAGENET_STD, inplace=False)\n","    )\n","    if not use_crop:\n","        transforms = transforms[1:]\n","    return torch.jit.script(transforms)\n","\n","valid_transforms = torch.jit.script(nn.Sequential(\n","    Normalize(IMAGENET_MEAN, IMAGENET_STD, inplace=False)\n","))\n","\n","\n","def train(config=None):\n","    with wandb.init(config=config, project=\"idl-aiornot\"):\n","        config = wandb.config\n","        print(config)\n","\n","        train_transforms = get_train_transforms(\n","            config.get(\"max_rotation\", 0), config.get(\"max_brightness\", 0.0),\n","            config.get(\"max_contrast\", 0.0), config.get(\"sharpness_p\", 0.0),\n","            config.get(\"flip_p\", 0.0), config.get(\"use_crop\", False)\n","        )\n","        model_name = config[\"model_name\"]\n","        pooling = config.get(\"pooling\", \"avg\")\n","        # ConvNeXt doesn't work with concat pooling\n","        if model_name.find(\"convnext_\") != -1:\n","            pooling = \"avg\"\n","        module = AIOrNotModule(\n","            model_name=model_name, global_pool=pooling, lr=config[\"lr\"],\n","            label_smoothing=config.get(\"label_smoothing\", 0.03),\n","            weight_decay=config.get(\"weight_decay\", 0.01),\n","            \n","            train_transforms=train_transforms, valid_transforms=valid_transforms\n","        )\n","        datamodule = AIOrNotDataModule(PREPROCESSED, batch_size=32)\n","\n","        \n","        wandb_logger = WandbLogger(log_model=LOG_CHECKPOINTS)\n","        callbacks = []\n","        if LOG_CHECKPOINTS:\n","            callbacks.append(pl.callbacks.ModelCheckpoint(monitor=\"val_BinaryAUROC\", mode=\"max\"))\n","\n","        trainer = pl.Trainer(\n","            gradient_clip_val=config[\"gradient_clip_value\"],\n","            precision=16, benchmark=True,\n","            accelerator=\"gpu\" if use_gpu else \"tpu\",\n","            auto_scale_batch_size=\"power\",\n","            auto_lr_find=False,\n","            detect_anomaly=False,\n","            devices=-1,\n","            max_epochs=config[\"epochs\"],\n","            log_every_n_steps=10,\n","            logger=wandb_logger,\n","            callbacks=callbacks\n","        )\n","\n","        trainer.tune(model=module, datamodule=datamodule)\n","        trainer.fit(model=module, datamodule=datamodule)\n","\n","        # In case we are using runtime hyperparameter tuning, we might need to update them for logging\n","        wandb.config.update(module.hparams)\n","        wandb.config.update(datamodule.hparams)\n","        print(wandb.config)\n","        \n","    # Force memory clearing\n","    del trainer\n","    del module\n","    del datamodule\n","    del wandb_logger\n","    del train_transforms\n","    gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Dataset and Lightning Data Module"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AIOrNotDataset(torch.utils.data.Dataset):\n","    def __init__(self, images, targets, normalize=True):\n","        self.images = images\n","        self.targets = targets\n","        self.normalize = normalize\n","    \n","    def __len__(self):\n","        return len(self.targets)\n","    \n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        if self.normalize:\n","            image = preprocess_image(image)\n","        else:\n","            image = to_tensor(image)\n","        return (image, torch.tensor((self.targets[idx],), dtype=torch.float32))\n","\n","\n","class AIOrNotDataModule(pl.LightningDataModule):\n","    def __init__(self, path, batch_size):\n","        super().__init__()\n","        self.path = path\n","        self.batch_size = batch_size\n","        self.save_hyperparameters(ignore=[\"path\"])\n","        \n","    def setup(self, stage):\n","        self.fd = h5py.File(self.path)\n","        if stage == \"fit\":\n","            train_group = self.fd[\"train\"]\n","            valid_group = self.fd[\"valid\"]\n","            train_images = train_group[\"image\"]\n","            valid_images = valid_group[\"image\"]\n","            train_targets = train_group[\"target\"]\n","            valid_targets = valid_group[\"target\"]\n","            self.train_dataset = AIOrNotDataset(train_images, train_targets)\n","            self.valid_dataset = AIOrNotDataset(valid_images, valid_targets)\n","        elif stage == \"test\":\n","            test_group = self.fd[\"test\"]\n","            test_images = test_group[\"image\"]\n","            test_targets = test_group[\"target\"]\n","            self.test_dataset = AIOrNotDataset(test_images, test_targets)\n","            \n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=1,\n","            pin_memory=True,\n","            drop_last=True,\n","        )\n","    \n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.valid_dataset,\n","            batch_size=2*self.batch_size,\n","            shuffle=False,\n","            num_workers=1,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","    \n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.test_dataset,\n","            batch_size=2*self.batch_size,\n","            shuffle=False,\n","            num_workers=1,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","            \n","\n","    def teardown(self, stage):\n","        self.fd.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Lightning Module"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class AIOrNotModule(pl.LightningModule):\n","    def __init__(\n","        self, model_name, global_pool, lr, label_smoothing, weight_decay, num_classes=1,\n","        train_transforms=None, valid_transforms=None\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name,\n","            pretrained=True,\n","            num_classes=num_classes,\n","            global_pool=global_pool\n","        )\n","#         self.loss_func = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n","        self.loss_func = nn.BCEWithLogitsLoss()\n","        self.label_smoothing = label_smoothing\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","        self.save_hyperparameters(ignore=[\n","            \"model\", \"num_classes\", \"train_transforms\", \"valid_transforms\"\n","        ])\n","        task = \"binary\"\n","        metrics = torchmetrics.MetricCollection([\n","            torchmetrics.Accuracy(num_classes=num_classes, task=task),\n","            torchmetrics.Precision(num_classes=num_classes, task=task),\n","            torchmetrics.Recall(num_classes=num_classes, task=task),\n","            torchmetrics.AUROC(num_classes=num_classes, task=task),\n","        ])\n","        self.train_metrics = metrics.clone(prefix=\"train_\")\n","        self.val_metrics = metrics.clone(prefix=\"val_\")\n","        self.test_metrics = metrics.clone(prefix=\"test_\")\n","        self.train_transforms = train_transforms\n","        self.valid_transforms = valid_transforms\n","        \n","    def forward(self, x, transform=None):\n","        if transform is not None:\n","            x = transform(x)\n","        x = self.model(x)\n","        return x\n","    \n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x, self.train_transforms)\n","        loss = self.loss_func(y_hat, y)\n","        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","        self.log_dict(self.train_metrics(y_hat, y))\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x, self.valid_transforms)\n","        loss = self.loss_func(y_hat, y)\n","        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","        self.log_dict(self.val_metrics(y_hat, y))\n","        return loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x, self.valid_transforms)\n","        loss = self.loss_func(y_hat, y)\n","        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n","        self.log_dict(self.test_metrics(y_hat, y))\n","        return loss\n","    \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            optimizer, max_lr=self.lr,\n","            total_steps=self.trainer.estimated_stepping_batches,\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": lr_scheduler,\n","                \"interval\": \"step\",\n","                \"frequency\": 1,\n","            }\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With EDA done, we can now create the splits. Since the dataset is fairly balanced, we can just randomly split it into train (70%), validation (10%) and test (20%) sets, while keeping the same distribution of labels. One thing to note is that the dataset is too large to fit into memory, so, we need to use a streaming approach. We are going to process the data in batches and save the splits on disk using hdf5 format. During training, we are going to load the data from these files on demand."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.489889Z","iopub.status.busy":"2023-04-18T15:33:13.489233Z","iopub.status.idle":"2023-04-18T15:33:13.531264Z","shell.execute_reply":"2023-04-18T15:33:13.530289Z","shell.execute_reply.started":"2023-04-18T15:33:13.489854Z"},"trusted":true},"outputs":[],"source":["labels = split_data(labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The only preprocessing we are going to do before saving the data into hdf5 files is to decode them into numpy array and, optionally for the reduced mode, resize them to 256x256."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:33:13.544674Z","iopub.status.busy":"2023-04-18T15:33:13.544262Z","iopub.status.idle":"2023-04-18T15:35:11.427481Z","shell.execute_reply":"2023-04-18T15:35:11.426209Z","shell.execute_reply.started":"2023-04-18T15:33:13.544636Z"},"trusted":true},"outputs":[],"source":["preprocess_splits()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:11.429812Z","iopub.status.busy":"2023-04-18T15:35:11.428988Z","iopub.status.idle":"2023-04-18T15:35:12.509451Z","shell.execute_reply":"2023-04-18T15:35:12.507861Z","shell.execute_reply.started":"2023-04-18T15:35:11.429763Z"},"trusted":true},"outputs":[],"source":["!ls -lh /kaggle/scratch"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:12.514674Z","iopub.status.busy":"2023-04-18T15:35:12.513959Z","iopub.status.idle":"2023-04-18T15:35:13.001773Z","shell.execute_reply":"2023-04-18T15:35:12.997346Z","shell.execute_reply.started":"2023-04-18T15:35:12.514618Z"},"trusted":true},"outputs":[],"source":["if not INTERACTIVE_MODE and not DEBUG_MODE:\n","    del labels\n","    del images_table\n","    del data_table\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline Model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, we are going to try a baseline model. For training, I am using PyTorch Lightning with torchvision for basic preprocessing. Logging is done using Weights & Biases.\n","\n","The baseline model is a simple pretrained ResNet-18 trained with a bit of label smoothing and weight decay. The batch size and learning rate are automatically tuned (batch size by maximizing the GPU memory and learning rate by using [the learning rate finder](https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#learning-rate-finder))."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:14.421729Z","iopub.status.busy":"2023-04-18T15:35:14.421032Z","iopub.status.idle":"2023-04-18T15:35:14.516470Z","shell.execute_reply":"2023-04-18T15:35:14.515089Z","shell.execute_reply.started":"2023-04-18T15:35:14.421691Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE:\n","    print(timm.list_models(pretrained=True))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:14.518964Z","iopub.status.busy":"2023-04-18T15:35:14.518230Z","iopub.status.idle":"2023-04-18T15:35:14.613892Z","shell.execute_reply":"2023-04-18T15:35:14.611968Z","shell.execute_reply.started":"2023-04-18T15:35:14.518923Z"},"trusted":true},"outputs":[],"source":["if INTERACTIVE_MODE and not SWEEP_MODE:\n","    hyperparams = {\n","        \"pooling\": \"avg\",\n","        \"lr\": 0.001,\n","        \"label_smoothing\": 0.05,\n","        \"weight_decay\": 0.03,\n","        \"epochs\": 5,\n","        \"batch_size\": 32,\n","        \"gradient_clip_value\": None,\n","        \"model_name\": \"resnet18\",\n","    }\n","\n","\n","    module = AIOrNotModule(\n","        model_name=hyperparams[\"model_name\"], global_pool=hyperparams[\"pooling\"], lr=hyperparams[\"lr\"],\n","        label_smoothing=hyperparams[\"label_smoothing\"],\n","        weight_decay=hyperparams[\"weight_decay\"]\n","    )\n","    datamodule = AIOrNotDataModule(PREPROCESSED, batch_size=hyperparams[\"batch_size\"])\n","\n","    wandb_logger = WandbLogger(project=\"idl-aiornot\")\n","    # Add debug tag if running in DEBUG_MODE to filter those runs out\n","    if DEBUG_MODE:\n","        wandb_logger.experiment.tags += (\"debug\",)\n","    trainer = pl.Trainer(\n","        gradient_clip_val=hyperparams[\"gradient_clip_value\"],\n","        precision=16, benchmark=True,\n","        accelerator=\"gpu\" if use_gpu else \"cpu\",\n","        auto_scale_batch_size=\"power\",\n","        auto_lr_find=True,\n","        detect_anomaly=True,\n","        devices=-1 if use_gpu else None,\n","        max_epochs=hyperparams[\"epochs\"],\n","        log_every_n_steps=1 if DEBUG_MODE else 10,\n","        logger=wandb_logger\n","    )\n","\n","    trainer.tune(model=module, datamodule=datamodule)\n","    trainer.fit(model=module, datamodule=datamodule)\n","\n","    # In case we are using runtime hyperparameter tuning, we might need to update them for logging\n","    wandb.config.update(module.hparams)\n","    wandb.config.update(datamodule.hparams)\n","\n","    wandb.finish()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["|Split|AUROC|Accuracy|Precision|Recall|Loss|\n","|---|---|---|---|---|---|\n","|Train|0.9999|1.0000|1.0000|1.0000|0.0157|\n","|Val.|0.9982|0.9753|0.9683|0.9872|0.0558|"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The baseline model achieves really good results and as expected recall is higher than precision. However, the model is definitely overfitting too much since it has perfect accuracy on the train set. Precision suffers more from this overfitting, which might mean that compared to generated images, real images are more diverse and thus harder to classify."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["One thing to note is that training on a full dataset takes a lot of time. In order to speed up the iteration process, I am going to be using a reduced dataset. The reduced dataset is created by resizing the images to 256x256 and taking only 50% of the data. After doing a hyperparameter search on the reduced dataset, I am going to select a few candidates and train them on the full dataset to do a final evaluation."]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter Optimization"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The baseline works fairly well and there doesn't seem to be any problems with training. However, there is still a lot of room for improvement. We are going to take an easy route and do hyperparameter optimization. We are going to use Weights and Biases sweeps for that. It has an easy way to set up search spaces, which we are going to use for starting several levels of sweeps with more refined search spaces each time."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["An example of a sweep config used is shown below. For the HP search, I did a few sweeps with different search spaces:\n","\n","1. A random search without data augmentation\n","2. A random search with more tight bounds for the previous search space and data augmentation\n","3. A bayesian search with the more refined search space as the previous one\n","4. A random with only a few parameters to tune and data augmentation"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:15.197471Z","iopub.status.busy":"2023-04-18T15:35:15.197073Z","iopub.status.idle":"2023-04-18T15:35:15.269903Z","shell.execute_reply":"2023-04-18T15:35:15.268441Z","shell.execute_reply.started":"2023-04-18T15:35:15.197432Z"},"trusted":true},"outputs":[],"source":["sweep_config = {\n","    \"method\": \"random\",\n","    \"name\": \"hp_tuning_reduced_with_transforms\",\n","    \"metric\": {\n","        \"goal\": \"maximize\",\n","        \"name\": \"val_BinaryAUROC\"\n","    },\n","    \"parameters\": {\n","        \"pooling\": {\n","            \"values\": [\"avg\", \"catavgmax\"]\n","        },\n","        \"lr\": {\n","            \"distribution\": \"log_uniform_values\",\n","            \"min\": 1e-5,\n","            \"max\": 1e-2\n","        },\n","        \"label_smoothing\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0,\n","            \"max\": 0.1\n","        },\n","        \"weight_decay\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.01,\n","            \"max\": 0.2\n","        },\n","        \"epochs\": {\n","            \"distribution\": \"int_uniform\",\n","            \"min\": 2,\n","            \"max\": 20\n","        },\n","        \"gradient_clip_value\": {\n","            \"values\": [None, 0.5, 2.0, 5.0, 10.0]\n","        },\n","        \"model_name\": {\n","            \"values\": [\n","                \"resnet18\", \"resnet50\",\n","                \"convnext_tiny_in22k\", \"convnext_small_in22k\",\n","                \"efficientnet_b1\",\n","            ]\n","        },\n","        \"max_rotation\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0,\n","            \"max\": 45\n","        },\n","        \"max_brightness\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0,\n","            \"max\": 0.4\n","        },\n","        \"max_contrast\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0,\n","            \"max\": 0.4\n","        },\n","        \"sharpness_p\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0,\n","            \"max\": 0.6\n","        },\n","        \"flip_p\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0,\n","            \"max\": 0.5\n","        },\n","        \"use_crop\": {\n","            \"values\": [\"False\", \"True\"]\n","        }\n","    }\n","}\n","\n","if SWEEP_MODE:\n","    if sweep_id is None:\n","        sweep_id = wandb.sweep(sweep_config, project=\"idl-aiornot\")"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:15.278709Z","iopub.status.busy":"2023-04-18T15:35:15.278330Z","iopub.status.idle":"2023-04-18T15:35:15.358571Z","shell.execute_reply":"2023-04-18T15:35:15.357346Z","shell.execute_reply.started":"2023-04-18T15:35:15.278667Z"},"trusted":true},"outputs":[],"source":["if SWEEP_MODE:\n","    wandb.agent(sweep_id, train, count=5, project=\"idl-aiornot\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["After doing these 4 sweeps (totaling 128 models trained over ~13 hours), I narrowed down on 6 possible candidates. They are not that different, but that is to be expected considering the performance of the baseline model and the fact that the reduced dataset was used."]},{"cell_type":"markdown","metadata":{},"source":["## Final Model Training"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["With the candidate models selected, we can now train them on the full dataset. We are going to use the same hyperparameters by filtering the runs with `reduced_candidate` tag, getting the training configs and using them to train the models on the full dataset.\n","\n","After that we can decide on the final model to use and retrain it again to check if the results are reproducible and to save the final model artifact."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:15.444416Z","iopub.status.busy":"2023-04-18T15:35:15.443812Z","iopub.status.idle":"2023-04-18T15:35:15.511259Z","shell.execute_reply":"2023-04-18T15:35:15.509624Z","shell.execute_reply.started":"2023-04-18T15:35:15.444375Z"},"trusted":true},"outputs":[],"source":["# manually chosen runs with best val metrics and lowest overfitting\n","if TRAIN_CANDIDATES_MODE:\n","    wandb_api = wandb.Api()\n","    candidate_runs = wandb_api.runs(filters={\"tags\": \"reduced_candidate\"}, path=\"daniilgaltsev/idl-aiornot\")\n","    candidate_configs = [run.config for run in candidate_runs]"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:15.514382Z","iopub.status.busy":"2023-04-18T15:35:15.513754Z","iopub.status.idle":"2023-04-18T15:35:15.598072Z","shell.execute_reply":"2023-04-18T15:35:15.596896Z","shell.execute_reply.started":"2023-04-18T15:35:15.514343Z"},"trusted":true},"outputs":[],"source":["if TRAIN_CANDIDATES_MODE:\n","    for config in candidate_configs:\n","        train(config=config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Training those models took about ~4.5 hours, which underlines how much of a speedup the reduced dataset provides. If hyperparameter search was done on the full dataset, it would take 128/6*13h = ~277 hours instead of ~13.\n","\n","In any case, all thse candidate models perform very well with validation AUROC around 0.9985. I have decided to choose the candidate with the lowest amount of overfitting, which is a convnext_tiny model training only for 2 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:35:38.056977Z","iopub.status.busy":"2023-04-18T15:35:38.055897Z","iopub.status.idle":"2023-04-18T15:52:57.007044Z","shell.execute_reply":"2023-04-18T15:52:57.005963Z","shell.execute_reply.started":"2023-04-18T15:35:38.056931Z"},"trusted":true},"outputs":[],"source":["if TRAIN_FINAL_MODE:\n","    candidate_run_id = \"u4de23v3\"\n","    wandb_api = wandb.Api()\n","    run = wandb_api.run(path=f\"daniilgaltsev/idl-aiornot/{candidate_run_id}\")\n","    LOG_CHECKPOINTS = True\n","    train(config=run.config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This training run didn't go as well as the previous ones since there was some instability in the training process towards the end, but I decided to use it anyway because this is the inherent stochasticity of the training process and it is important to take it into account and try to fix it properly in the future."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since we have saved the best model checkpoint, we can load it and use it for inference and to get the test set metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:53:44.239411Z","iopub.status.busy":"2023-04-18T15:53:44.238900Z","iopub.status.idle":"2023-04-18T15:54:38.841842Z","shell.execute_reply":"2023-04-18T15:54:38.840720Z","shell.execute_reply.started":"2023-04-18T15:53:44.239367Z"},"trusted":true},"outputs":[],"source":["if TEST_FINAL_MODE:\n","    final_id = \"zu9wrq3b\"\n","    wandb_api = wandb.Api()\n","    run = wandb_api.run(path=f\"daniilgaltsev/idl-aiornot/{final_id}\")\n","    artifact = wandb_api.artifact(f\"daniilgaltsev/idl-aiornot/model-{final_id}:latest\")\n","    model_path = artifact.checkout()\n","\n","    model = AIOrNotModule.load_from_checkpoint(model_path + \"/\" + \"model.ckpt\")\n","    datamodule = AIOrNotDataModule(PREPROCESSED, batch_size=run.config[\"batch_size\"])\n","\n","    with wandb.init(resume=True, id=final_id, project=\"idl-aiornot\"):\n","        wandb_logger = WandbLogger()\n","        trainer = pl.Trainer(\n","            precision=16, benchmark=True,\n","            accelerator=\"gpu\" if use_gpu else \"tpu\",\n","            devices=-1,\n","            logger=wandb_logger,\n","            log_every_n_steps=1\n","        )\n","        trainer.test(model=model, datamodule=datamodule)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["|Metric|Baseline Model| Final Model|\n","|---|---|---|\n","|Train AUROC|**0.9999**|0.9960|\n","|Val. AUROC|**0.9982**|0.9977|\n","|Train Accuracy|**1.0000**|0.9688|\n","|Val. Accuracy|0.9753|**0.9780**|\n","|Train Precision|**1.0000**|0.9500|\n","|Val. Precision|0.9683|**0.9761**|\n","|Train Recall|**1.0000**|**1.0000**|\n","|Val. Recall|**0.9872**|0.9829|\n","|Train Loss|**0.0157**|0.0813|\n","|Val. Loss|**0.0558**|0.0584|"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["| Test Metric | Manual Benchmark | Final Model |\n","|--------|-----------|-------------|\n","|Accuracy|0.8400|**0.9296**|\n","| Recall |0.9545|**0.9990**|\n","|Precision|0.7500|**0.8882**|\n","|  AUROC |N/A|0.9962|\n","|  Loss  |N/A|0.1678|"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The tables above show the performance of the final model compared to the baseline model and the manual benchmark.\n","\n","Compared to the baseline model, the final model only has better accuracy and precision on the validation set. However, it does seem to have lower overfitting, which is a good sign. Since we know that the baseline model is overfitting, we can expect it to perform worse on the test set than expected, but I didn't check that. Another thing to note is that we know that the final model had some instability during training, so, it is possible to further improve it by adding a bit of regularization, doing better weight initilization for the final layer or doing gradual unfreezing.\n","\n","Compared to the manual benchmark, the performance is much better. They both follow the same pattern of being much better at recognizing generated images as generated, but precision still suffers in both cases. This once again shows that real images are harder to generalize on and the next step would be to try to improve the model by using more real images or coming up with a better way to augment them."]},{"cell_type":"markdown","metadata":{},"source":["# Discussions"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this project we tried to detect if images are generated or real. From the experiments it can be concluded that this task is not difficult for both humans and deep learning models, especially when it's more important to detect that generated images are generated. The final model performance is shown below.\n","\n","|Test Metric|Value |\n","|-----------|------|\n","| Accuracy  |0.9296|\n","|  Recall   |0.9990|\n","| Precision |0.8882|\n","|   AUROC   |0.9962|\n","\n","The table shows that the main type of error the model is making is labeling real images as generated, which also mirrors the results of the manual benchmark. This means that further work should be focused on improving the model's precision. It's also important to note that the final model experienced some instability during training, which might have negatively affected the results.\n","\n","The main problem with the experiment procedure was that the  the reduced dataset might have been too small compared to batch sizes used. This reduced the possible number of training steps, which might have skewed the results of hyperparameter tuning. This could have been fixed by manually selected a batch size to use instead of using the maximum possible one.\n","\n","In order to further improve performance several steps are possible. First, more real images can be collected. This shouldn't be too difficult since they are a lot of publicly available images. The only concern is to make sure that they are not generated, which can be done by collecting only images before a certain data (for example, images from 2019 and earlier). Secondly, training stability can be improved by using better weight initialization for classification layer and adding gradual unfreezeing while finetuning the model. Also, the root cause of the instability should be investigated by inspecting gradients and activation histograms throughout the training process. Finally, the errors that the model is making should be analyzed to see if there are any patterns to those mistake and if they can be incorporated into the data augmentation process."]},{"cell_type":"markdown","metadata":{},"source":["# Contributions and Takeaways"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The project's goal was to train a model capable of differentiating between real and generated images. For that a dataset of real and AI images was used. A manual benchmark was performed to establish what kind of performance can be expected from an untrained person. The benchmark showed that the task is not difficult and that the modern deep learning techniques should be capable of solving the problem at a decent level. The modelling itself was done using PyTorch Lightning and Weights & Biases. The data was transformed from parquet files into hdf5 files with preprocessed images and targets in NumPy arrays. Over a course of 4 hyperparameter sweeps several model candidates were established. The final model then was trained and its results evaluated on the test set. The final model was able to achieve almost perfect recall, but still has a lot of ways to improve its precision."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
